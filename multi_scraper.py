"""
R√¥le global :
Ce module est con√ßu pour effectuer un scraping multi-site en parall√®le, en utilisant les capacit√©s de multiprocessing. 
Il combine des approches dynamiques et statiques pour extraire les donn√©es des sites web, puis applique une analyse s√©mantique 
pour enrichir les r√©sultats. Enfin, il g√©n√®re un rapport PDF et sauvegarde les donn√©es dans un fichier JSON.

Pourquoi il est important :
Dans le pipeline global (scraping ‚Üí analyse ‚Üí visualisation ‚Üí rapport), ce module est essentiel car il automatise le traitement 
de plusieurs sites en parall√®le, r√©duisant ainsi le temps n√©cessaire pour collecter et analyser les donn√©es. Il garantit √©galement 
une gestion robuste des erreurs et des fallback (retours en arri√®re) pour maximiser le taux de succ√®s.

Comment il aide dans le pipeline :
- **Scraping** : Extrait les donn√©es des sites web en utilisant des approches dynamiques (JavaScript) et statiques (HTML brut).
- **Analyse** : Enrichit les donn√©es extraites avec une analyse s√©mantique pour structurer les informations.
- **Visualisation** : G√©n√®re un rapport PDF contenant les r√©sultats du scraping.
- **Rapport** : Sauvegarde les donn√©es dans un fichier JSON pour une utilisation ult√©rieure.

Technologies utilis√©es :
- **Multiprocessing** : Pour ex√©cuter le scraping en parall√®le sur plusieurs sites.
- **Logging** : Pour suivre les √©tapes du processus et g√©rer les erreurs.
- **JSON** : Pour sauvegarder les r√©sultats structur√©s.
- **FPDF** : Pour g√©n√©rer un rapport PDF.
- **Selenium** et **BeautifulSoup** : Pour le scraping dynamique et statique.
"""

import time
import logging
from multiprocessing import Pool, cpu_count
from scraping.scraper_dynamic import scrape_dynamic_site
from scraping.scraper_static import scrape_static_site
from utils.io_handler import load_sites, save_json
from utils.pdf_report import generate_pdf
from analyse.analyseur_semantique import analyse_semantique_site as analyser_semantiquement

SITE_TIMEOUT = 150  # secondes max par site

# Configuration du logger pour suivre les √©tapes du processus
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler('scraper.log', encoding='utf-8'), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

def process_result(raw_result, site_meta):
    """
    R√¥le :
    Traite les r√©sultats bruts du scraping pour les enrichir avec des m√©tadonn√©es et une analyse s√©mantique.

    Fonctionnalit√© :
    - Fusionne les donn√©es brutes avec les m√©tadonn√©es du site.
    - Applique une analyse s√©mantique pour structurer et enrichir les r√©sultats.
    - G√®re les retours en arri√®re (fallback) en cas d'√©chec du scraping dynamique.

    Importance :
    Cette fonction garantit que les r√©sultats sont complets, enrichis et pr√™ts pour les √©tapes suivantes du pipeline.

    Arguments :
    - `raw_result` : Les donn√©es brutes extraites du site.
    - `site_meta` : Les m√©tadonn√©es du site (nom, URL).

    Retour :
    Un dictionnaire contenant les r√©sultats enrichis.
    """
    if not isinstance(raw_result, dict):
        raw_result = {"success": False, "error": "invalid result"}

    if "data" in raw_result and isinstance(raw_result["data"], dict):
        merged = {**raw_result, **raw_result["data"]}
        merged.pop("data", None)
        raw_result = merged

    raw_result["name"] = site_meta.get("name", raw_result.get("name", ""))
    raw_result["url"] = site_meta.get("url", raw_result.get("url", ""))

    if not raw_result.get("success"):
        logger.info(f"‚ÑπÔ∏è Fallback static for {raw_result.get('url')}")
        static = scrape_static_site(raw_result.get("url"))
        if static and static.get("success"):
            raw_result = static
            if "data" in raw_result and isinstance(raw_result["data"], dict):
                merged = {**raw_result, **raw_result["data"]}
                merged.pop("data", None)
                raw_result = merged

    data_input = raw_result if raw_result.get("success") else raw_result.get("data", raw_result)

    try:
        sem = analyser_semantiquement(data_input, url=raw_result.get("url", site_meta.get("url", "")))
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è analyse_semantique failed for {raw_result.get('url')}: {e}")
        sem = data_input

    return {
        "name": sem.get("name", site_meta.get("name", "")),
        "url": sem.get("url", site_meta.get("url", "")),
        "success": True if raw_result.get("success") or sem else False,
        **sem
    }

def scrape_site_worker(site: dict) -> dict:
    """
    R√¥le :
    Effectue le scraping d'un site en utilisant une approche dynamique, avec fallback statique en cas d'√©chec.

    Fonctionnalit√© :
    - Tente d'extraire les donn√©es dynamiquement.
    - Passe √† une approche statique si le scraping dynamique √©choue.
    - Retourne les r√©sultats enrichis avec les m√©tadonn√©es du site.

    Importance :
    Cette fonction garantit que chaque site est trait√© de mani√®re robuste, m√™me en cas d'√©chec partiel.

    Arguments :
    - `site` : Un dictionnaire contenant les informations du site (nom, URL).

    Retour :
    Un dictionnaire contenant les r√©sultats du scraping.
    """
    name = site.get("name", "Nom inconnu")
    url = site.get("url", "")
    logger.info(f"üîç Scraping: {name} ({url})")
    try:
        raw = scrape_dynamic_site(url)
        return process_result(raw, site)
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du scraping de {name}: {e}")
        try:
            static = scrape_static_site(url)
            return process_result(static, site)
        except Exception as e2:
            logger.error(f"‚ùå Static fallback failed for {name}: {e2}")
            return {"name": name, "url": url, "success": False, "error": str(e2)}

def main():
    """
    R√¥le :
    Point d'entr√©e principal pour lancer le scraping multi-site.

    Fonctionnalit√© :
    - Charge la liste des sites √† scraper.
    - Ex√©cute le scraping en parall√®le en utilisant plusieurs processus.
    - Sauvegarde les r√©sultats dans un fichier JSON et g√©n√®re un rapport PDF.

    Importance :
    Cette fonction orchestre l'ensemble du processus de scraping, garantissant une ex√©cution efficace et une gestion robuste des erreurs.

    √âtapes :
    1. Charge les sites depuis un fichier CSV.
    2. Configure le multiprocessing pour traiter les sites en parall√®le.
    3. Sauvegarde les r√©sultats et g√©n√®re un rapport PDF.

    Retour :
    Aucun retour. Les r√©sultats sont sauvegard√©s et affich√©s dans la console.
    """
    logger.info("üöÄ Lancement du scraping multi-site...")
    try:
        sites = load_sites("sites.csv")
    except Exception as e:
        logger.error(f"‚ùå Erreur chargement sites.csv : {e}")
        return

    if not sites:
        logger.warning("‚ö†Ô∏è Aucun site trouv√©.")
        return

    logger.info(f"üì• {len(sites)} sites √† traiter")
    n_processes = max(cpu_count() - 1, 1)
    logger.info(f"‚öôÔ∏è Utilisation de {n_processes} processus")

    start = time.time()
    results = []

    with Pool(processes=n_processes) as pool:
        async_results = [pool.apply_async(scrape_site_worker, (site,)) for site in sites]
        for i, async_res in enumerate(async_results, start=1):
            try:
                result = async_res.get(timeout=SITE_TIMEOUT)
                results.append(result)
            except Exception as e:
                logger.error(f"‚è±Ô∏è Timeout ou erreur pour site {sites[i-1].get('url')}: {e}")
                results.append({"name": sites[i-1].get("name", ""), "url": sites[i-1].get("url", ""), "success": False, "error": "Timeout ou erreur"})

    duration = time.time() - start
    logger.info(f"‚è±Ô∏è Temps total : {duration:.2f}s")

    save_json(results, "resultats.json")
    logger.info("‚úÖ Donn√©es sauvegard√©es dans resultats.json")

    if generate_pdf(results, "rapport_sites.pdf"):
        logger.info("üìÑ Rapport PDF g√©n√©r√© avec succ√®s")
    else:
        logger.error("‚ùå √âchec de g√©n√©ration du PDF")

    logger.info("üèÅ Traitement termin√©.")

if __name__ == "__main__":
    main()
