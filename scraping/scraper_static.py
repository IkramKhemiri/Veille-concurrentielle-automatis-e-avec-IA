"""
R√¥le global :
Ce module est con√ßu pour scraper des sites web statiques, c'est-√†-dire des sites dont le contenu est directement accessible via des requ√™tes HTTP. 
Il extrait les donn√©es pertinentes √† partir du HTML brut et g√®re √©galement la pagination pour collecter des informations sur plusieurs pages.

Pourquoi il est important :
Dans le pipeline global (scraping ‚Üí analyse ‚Üí visualisation ‚Üí rapport), ce module est essentiel pour traiter les sites statiques, 
qui repr√©sentent une part importante du web. Il permet de collecter rapidement et efficacement des donn√©es exploitables sans n√©cessiter 
de rendu JavaScript.

Comment il aide dans le pipeline :
- **Scraping** : Il extrait le contenu HTML brut des sites statiques.
- **Analyse** : Les donn√©es extraites sont nettoy√©es et organis√©es pour √™tre analys√©es dans les √©tapes suivantes.
- **Visualisation** : Les informations collect√©es peuvent √™tre utilis√©es pour cr√©er des graphiques ou des tableaux.
- **Rapport** : Les donn√©es structur√©es sont pr√™tes √† √™tre int√©gr√©es dans des rapports professionnels.

Technologies utilis√©es :
- **Requests** : Pour envoyer des requ√™tes HTTP et r√©cup√©rer le contenu HTML des pages.
- **BeautifulSoup** : Pour analyser et extraire les donn√©es textuelles des pages HTML.
- **Expressions r√©guli√®res (regex)** : Pour d√©tecter et extraire des motifs sp√©cifiques comme les emails et les num√©ros de t√©l√©phone.
- **Logging** : Pour suivre les √©tapes du processus et g√©rer les erreurs de mani√®re transparente.
"""

import logging
import requests
from scraping.extractor import extract_all
from scraping.cleaner import clean_extracted_data
from urllib.parse import urlparse, urlunparse, parse_qs, urlencode

def get_static_html(url, timeout=10):
    """
    R√¥le :
    R√©cup√®re le contenu HTML brut d'un site web statique via une requ√™te HTTP.

    Fonctionnalit√© :
    - Envoie une requ√™te HTTP GET √† l'URL sp√©cifi√©e.
    - Retourne le contenu HTML si la requ√™te est r√©ussie.
    - G√®re les erreurs et les statuts HTTP non 200.

    Importance :
    Cette fonction est id√©ale pour scraper des sites statiques qui ne n√©cessitent pas de rendu JavaScript. 
    Elle est rapide et consomme moins de ressources qu'un navigateur automatis√©.

    Arguments :
    - `url` : L'URL du site √† scraper.
    - `timeout` : Temps maximum (en secondes) pour la requ√™te.

    Retour :
    Une cha√Æne de caract√®res contenant le HTML brut ou une cha√Æne vide en cas d'erreur.
    """
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        response = requests.get(url, headers=headers, timeout=timeout)
        if response.status_code == 200:
            return response.text
        else:
            logging.warning(f"‚ö†Ô∏è Erreur HTTP {response.status_code} pour {url}")
            return ""
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Erreur get_static_html : {e}")
        return ""

def is_empty(data):
    """
    R√¥le :
    V√©rifie si les donn√©es extraites sont vides ou non pertinentes.

    Fonctionnalit√© :
    - Parcourt les sections cl√©s des donn√©es extraites (services, clients, blog, etc.).
    - Retourne `True` si toutes les sections sont vides ou non significatives.

    Importance :
    Cette fonction permet de d√©tecter rapidement si le scraping a √©chou√© ou si les donn√©es extraites sont inutilisables.

    Arguments :
    - `data` : Un dictionnaire contenant les donn√©es extraites.

    Retour :
    `True` si les donn√©es sont vides, sinon `False`.
    """
    if not data or not isinstance(data, dict):
        return True
    for key in ["services", "clients", "blog", "jobs", "summary"]:
        val = data.get(key)
        if isinstance(val, str) and val.strip():
            return False
        elif isinstance(val, list) and any(str(x).strip() for x in val):
            return False
    return True

def merge_results(results):
    """
    R√¥le :
    Fusionne les r√©sultats extraits de plusieurs pages en un seul ensemble de donn√©es.

    Fonctionnalit√© :
    - Combine les donn√©es extraites en unifiant les listes et en conservant les valeurs uniques.

    Importance :
    Cette fonction est cruciale pour obtenir un ensemble de donn√©es complet et non dupliqu√© √† partir de plusieurs pages.

    Arguments :
    - `results` : Liste des blocs de r√©sultats extraits.

    Retour :
    Un dictionnaire contenant les donn√©es fusionn√©es.
    """
    merged = {}
    for block in results:
        for key, val in block.items():
            if isinstance(val, list):
                merged.setdefault(key, []).extend(val)
            elif isinstance(val, str):
                if not merged.get(key):
                    merged[key] = val
    return merged

def update_url_param(url, param_name, param_value):
    parts = urlparse(url)
    query = parse_qs(parts.query)
    query[param_name] = [str(param_value)]
    new_query = urlencode(query, doseq=True)
    new_parts = parts._replace(query=new_query)
    return urlunparse(new_parts)

def try_paginate(url):
    """
    R√¥le :
    G√®re la pagination pour collecter des donn√©es sur plusieurs pages d'un site statique.

    Fonctionnalit√© :
    - Modifie les param√®tres de l'URL pour acc√©der aux pages suivantes.
    - Extrait les donn√©es de chaque page et les fusionne.

    Importance :
    Cette m√©thode est essentielle pour scraper des sites statiques avec plusieurs pages de contenu.

    Arguments :
    - `url` : L'URL de base pour la pagination.

    Retour :
    Un dictionnaire contenant les donn√©es fusionn√©es de toutes les pages.
    """
    logging.info("üîÅ Pagination intelligente activ√©e...")
    results = []
    seen_urls = set()
    seen_contents = set()
    
    param_names = ["page", "p", "start", "offset"]
    current_page = 1
    stop = False

    while not stop:
        stop = True
        for param in param_names:
            paginated_url = update_url_param(url, param, current_page)
            if paginated_url in seen_urls:
                continue

            logging.info(f"üìÑ Scraping page {current_page} via param `{param}` : {paginated_url}")
            try:
                html = get_static_html(paginated_url)
                if not html or len(html) < 100:
                    continue

                html_clean = clean_extracted_data(html)
                data = extract_all(html_clean, paginated_url)

                content_signature = str(data)
                if is_empty(data) or content_signature in seen_contents:
                    continue

                seen_contents.add(content_signature)
                seen_urls.add(paginated_url)
                results.append(data)
                stop = False

            except Exception as e:
                logging.warning(f"‚ö†Ô∏è Erreur pagination sur {paginated_url}: {str(e)}")

        current_page += 1
        if current_page > 50:
            logging.warning("‚ö†Ô∏è Arr√™t forc√© √† 50 pages pour √©viter boucle infinie")
            break

    return merge_results(results)

def scrape_static_site(url: str) -> dict:
    """
    R√¥le :
    Point d'entr√©e principal pour le scraping des sites statiques.

    Fonctionnalit√© :
    - R√©cup√®re le contenu HTML brut d'une page.
    - Extrait les donn√©es pertinentes et g√®re la pagination si n√©cessaire.
    - Nettoie et structure les donn√©es extraites.

    Importance :
    Cette fonction orchestre l'ensemble du processus de scraping statique, garantissant que les donn√©es collect√©es sont exploitables.

    Arguments :
    - `url` : L'URL du site √† scraper.

    Retour :
    Un dictionnaire contenant les donn√©es extraites ou un message d'erreur en cas d'√©chec.
    """
    logging.info(f"üåê Scraping statique pour : {url}")

    try:
        html = get_static_html(url)
        if not html or len(html) < 100:
            raise ValueError("‚ùå HTML trop court ou vide")

        html_clean = clean_extracted_data(html)
        first_data = extract_all(html_clean, base_url=url)

        paginated_results = try_paginate(url)
        all_results = merge_results([first_data, paginated_results])

        for key in ["summary", "slogan", "location"]:
            if key in all_results and isinstance(all_results[key], str):
                all_results[key] = clean_extracted_data(all_results[key])

        if is_empty(all_results):
            raise ValueError("‚ùå Contenu vide ou sans valeur apr√®s extraction")

        return {
            "success": True,
            "url": url,
            "data": all_results
        }

    except Exception as e:
        logging.error(f"‚õî Erreur finale sur {url}: {e}")
        return {
            "success": False,
            "url": url,
            "error": str(e)
        }
